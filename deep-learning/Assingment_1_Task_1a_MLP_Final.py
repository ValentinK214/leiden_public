# -*- coding: utf-8 -*-
"""Assingment 1 - Task 1a MLP final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u5EjoXnmmOKM69CC2PoplYUa-ZyILA8l

# Read me

You can run this file without having to load any external data.

This notebook took about 1 hour to run.
"""

from __future__ import print_function

import keras
from keras.datasets import fashion_mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras import regularizers
import pandas as pd
import seaborn as sns

import matplotlib.pyplot as plt

import os
import tensorflow as tf
from tensorflow import keras
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.layers import Conv2D, GlobalAveragePooling2D, Dense
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import cifar10

"""# MLP Fashion MNIST"""

#Function to build models with diferent number of neurons, diferent number of layers and different regularizations
def build_model(n_neurons,i=0,n_hidden=1,):
  if i == 1:
    name = f'Model_using_adam_and_L2_regularization_with_{n_hidden}_hidden_layers'
  elif i == 2:
    name = f'Model_using_adam_and_Dropout_regularization_with_{n_hidden}_hidden_layers'
  else:
    name = f'Model_using_adam_and_no_regularization_with_{n_hidden}_hidden_layers'
  model = keras.models.Sequential(name=name)
  model.add(keras.layers.Flatten(input_shape=[28, 28]))
  for layer in range(n_hidden):
    if i == 2:
      model.add(keras.layers.Dropout(rate=0.2))
    if i == 1:
      model.add(keras.layers.Dense(n_neurons[layer], activation="relu",kernel_initializer="he_normal",kernel_regularizer=keras.regularizers.l2(0.01)))
    else:
      model.add(keras.layers.Dense(n_neurons[layer], activation="relu"))
  model.add(keras.layers.Dense(10, activation="softmax"))
  return model

num_classes = 10
epochs = 20
(X_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()


X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat",
 "Sandal", "Shirt", "Sneaker", "Bag", "Ankle boot"]

neurons = [[100],[100,300],[100,200,300,400,500]]
#All our models will have either no reg, L2 or Dropout
#We will try diferent number of neurons and number of layers as other hyperparameters tried like optimizer don't seem to affect much
models = []
for n in neurons:
  for i in range(3):
    models.append(build_model(n,i,len(n)))

for model in models:
  model.summary()

"""# 1 hidden layer models"""

#Models with 1 hidden layer, 100 neurons and the 3 regularization techniques
scores = []
for model in models[:3]:
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

  history = model.fit(X_train, y_train, epochs=epochs,validation_data=(X_valid, y_valid))

  score = model.evaluate(x_test, y_test, verbose=1)
  scores.append((score,model.name,model.count_params()))
  print('Test loss:', score[0])
  print('Test accuracy:', score[1])
  pd.DataFrame(history.history).plot(figsize=(8, 5))
  plt.grid(True)
  plt.title(model.name)
  plt.gca().set_ylim(0, 1)
  plt.show()

"""# 2 hidden layer models"""

#Models with 2 hidden layer, 100,300 neurons and the 3 regularization techniques
for model in models[3:6]:
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

  history = model.fit(X_train, y_train, epochs=20,validation_data=(X_valid, y_valid))

  score = model.evaluate(x_test, y_test, verbose=1)
  scores.append((score,model.name,model.count_params()))
  print('Test loss:', score[0])
  print('Test accuracy:', score[1])
  pd.DataFrame(history.history).plot(figsize=(8, 5))
  plt.grid(True)
  plt.title(model.name)
  plt.gca().set_ylim(0, 1)
  plt.show()

"""# 5 hidden layer models"""

#Models with 5 hidden layer, 100,200,300,400,500 neurons and the 3 regularization techniques
for model in models[6:]:
  model.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

  history = model.fit(X_train, y_train, epochs=epochs,validation_data=(X_valid, y_valid))

  score = model.evaluate(x_test, y_test, verbose=1)
  scores.append((score,model.name,model.count_params()))
  print('Test loss:', score[0])
  print('Test accuracy:', score[1])
  pd.DataFrame(history.history).plot(figsize=(8, 5))
  plt.grid(True)
  plt.title(model.name)
  plt.gca().set_ylim(0, 1)
  plt.show()

"""# Low parameters models"""

#Model with low number of parameters 8000 with only 1 hidden layer and 10 neurons
low_neurons = [10]
model_low_parameters = build_model(low_neurons,0,len(low_neurons))
model_low_parameters.summary()

model_low_parameters.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

history = model_low_parameters.fit(X_train, y_train, epochs=epochs,validation_data=(X_valid, y_valid))

score = model_low_parameters.evaluate(x_test, y_test, verbose=1)
scores.append((score,model_low_parameters.name,model_low_parameters.count_params()))
print('Test loss:', score[0])
print('Test accuracy:', score[1])
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.title(model_low_parameters.name)
plt.gca().set_ylim(0, 1)
plt.show()

#Model with even lower number of parameters 4000 with only 1 hidden layer and 5 neurons
lower_neurons = [5]
model_lower_parameters = build_model(lower_neurons,0,len(lower_neurons))
model_lower_parameters.summary()

model_lower_parameters.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

history = model_lower_parameters.fit(X_train, y_train, epochs=epochs,validation_data=(X_valid, y_valid))

score = model_lower_parameters.evaluate(x_test, y_test, verbose=1)
scores.append((score,model_lower_parameters.name,model_lower_parameters.count_params()))
print('Test loss:', score[0])
print('Test accuracy:', score[1])
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.title(model_lower_parameters.name)
plt.gca().set_ylim(0, 1)
plt.show()

#Model with lowest number of parameters 1600 with only 1 hidden layer and 2 neurons
lowest_neurons = [2]
model_lowest_parameters = build_model(lowest_neurons,0,len(lowest_neurons))
model_lowest_parameters.summary()

model_lowest_parameters.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

history = model_lowest_parameters.fit(X_train, y_train, epochs=epochs,validation_data=(X_valid, y_valid))

score = model_lowest_parameters.evaluate(x_test, y_test, verbose=1)
scores.append((score,model_lowest_parameters.name,model_lowest_parameters.count_params()))
print('Test loss:', score[0])
print('Test accuracy:', score[1])
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.title(model_lowest_parameters.name)
plt.gca().set_ylim(0, 1)
plt.show()

"""# High parameters models"""

#Model with high number of parameters, 5.8 million parameters 12 hidden layers
neurons_high = [100,200,300,400,500,600,700,800,900,1000,1100,1200]
model_high = build_model(neurons_high,0,len(neurons_high))
model_high.summary()

model_high.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

history = model_high.fit(X_train, y_train, epochs=epochs,validation_data=(X_valid, y_valid))

score = model_high.evaluate(x_test, y_test, verbose=1)
scores.append((score,model_high.name,model_high.count_params()))
print('Test loss:', score[0])
print('Test accuracy:', score[1])
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.title(model_high.name)
plt.show()

#Model with highest number of parameters, 11.3 million parameters 15 hidden layers
neurons_high = [100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500]
model_highest = build_model(neurons_high,0,len(neurons_high))
model_highest.summary()

model_highest.compile(loss='sparse_categorical_crossentropy',
                optimizer=Adam(),
                metrics=['accuracy'])

history = model_highest.fit(X_train, y_train, epochs=epochs,validation_data=(X_valid, y_valid))

score = model_highest.evaluate(x_test, y_test, verbose=1)
scores.append((score,model_highest.name,model_highest.count_params()))
print('Test loss:', score[0])
print('Test accuracy:', score[1])
pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.title(model_highest.name)
plt.show()

"""# MLP Fashion MNIST Results"""

df = pd.DataFrame(scores, columns=["Test","Name","Num params"])

df[["Test loss","Test accuracy"]] = pd.DataFrame(df['Test'].tolist(), index=df.index)
df.drop(columns=['Test'], inplace=True)
df.set_index(df['Name'] + ' (Params: ' + df['Num params'].astype(str) + ')', inplace=True)
df.drop(columns=['Name'], inplace=True)

reg = []
for i, row in df.iterrows():
  if 'L2' in row.name:
    reg.append(0)
  elif 'Dropout' in row.name:
    reg.append(1)
  else:
    reg.append(2)

df['Reg'] = reg
df = df.sort_values("Num params")

df

df.sort_values("Test accuracy")

palette = sns.color_palette("deep", n_colors=3)

marker_styles = {0: 'o', 1: 's', 2: 'D'}
plt.figure(figsize=(8, 5))
plt.plot(df['Num params'], df['Test loss'], alpha = 0.4)

for reg_value, color in enumerate(palette):
    mask = df['Reg'] == reg_value
    if reg_value == 2:
        label = 'No regularization'
    elif reg_value == 1:
        label = 'Dropout'
    else:
        label = 'L2 regularization'
    plt.scatter(df.loc[mask, 'Num params'], df.loc[mask, 'Test loss'],
                color=color, label=label, marker=marker_styles[reg_value])

plt.ylabel('Test Loss')
plt.xlabel('Number of Parameters (log)')
plt.xscale('log')
plt.title('Number of Parameters over Test Loss (Fashion MNIST)')
plt.legend()

plt.savefig('MLP_mnist_Ushape.png')

plt.show()

top_3_models = df.sort_values("Test accuracy",ascending=False)[:3]
top_3_models

df.to_csv('ModelsMLP.csv')

"""#3 best MLP models on CIFAR"""

(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0
x_val = x_train[:10000]
y_val = y_train[:10000]
x_train_final = x_train[10000:]
y_train_final = y_train[10000:]
print(
x_train.shape,
x_val.shape,
x_test.shape,
y_train.shape,
y_val.shape,
y_test.shape)
class_names_cifar = [
    'Airplane',    # 0
    'Automobile',  # 1
    'Bird',        # 2
    'Cat',         # 3
    'Deer',        # 4
    'Dog',         # 5
    'Frog',        # 6
    'Horse',       # 7
    'Ship',        # 8
    'Truck'        # 9
]

def plot_training_history(history):
    train_accuracy = history.history['accuracy']
    val_accuracy = history.history['val_accuracy']
    train_loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(1, len(train_accuracy) + 1)

    plt.figure(figsize=(12, 6))

    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_accuracy, 'bo-', label='Training Accuracy')
    plt.plot(epochs, val_accuracy, 'ro-', label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_loss, 'bo-', label='Training Loss')
    plt.plot(epochs, val_loss, 'ro-', label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid()

    plt.tight_layout()
    plt.show()


def test_accuracy(model,x_test,y_test,class_names):
    y_pred_prob= model.predict(x_test)
    y_pred= np.argmax(y_pred_prob, axis=1)
    conf_matrix = confusion_matrix(y_test, y_pred)
    print(conf_matrix)
    print(classification_report(y_test, y_pred, target_names=class_names))

mlp_cifar_1 = keras.models.Sequential([
    keras.layers.Input(shape=[32, 32, 3]),
    keras.layers.Flatten(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(300, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])
mlp_cifar_1.compile(optimizer='adam',
                     loss='sparse_categorical_crossentropy',
                     metrics=['accuracy'])
mlp_cifar_1.summary()

mlp_cifar_2 = keras.models.Sequential([
    keras.layers.Input(shape=[32, 32, 3]),
    keras.layers.Flatten(),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])
mlp_cifar_2.compile(optimizer='adam',
                     loss='sparse_categorical_crossentropy',
                     metrics=['accuracy'])
mlp_cifar_2.summary()

mlp_cifar_3 = keras.models.Sequential([
    keras.layers.Input(shape=[32, 32, 3]),
    keras.layers.Flatten(),
    keras.layers.Dropout(rate=0.2),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])

mlp_cifar_3.compile(optimizer='adam',
                    loss='sparse_categorical_crossentropy',
                    metrics=['accuracy'])
mlp_cifar_3.summary()

history_1=mlp_cifar_1.fit(x_train,y_train,epochs=epochs,validation_data=(x_val,y_val))

history_2=mlp_cifar_2.fit(x_train,y_train,epochs=epochs,validation_data=(x_val,y_val))

history_3=mlp_cifar_3.fit(x_train,y_train,epochs=epochs,validation_data=(x_val,y_val))

mlp_cifar_1.evaluate(x_test, y_test, verbose=1)

mlp_cifar_2.evaluate(x_test, y_test, verbose=1)

mlp_cifar_3.evaluate(x_test, y_test, verbose=1)

test_accuracy(mlp_cifar_1,x_test,y_test,class_names_cifar)

test_accuracy(mlp_cifar_2,x_test,y_test,class_names_cifar)

test_accuracy(mlp_cifar_3,x_test,y_test,class_names_cifar)

print("################################################ M L P 1 #################################  \n")
plot_training_history(history_1)
print("################################################ M L P 2 #################################  \n")
plot_training_history(history_2)
print("################################################ M L P 3 ################################# \n")
plot_training_history(history_3)